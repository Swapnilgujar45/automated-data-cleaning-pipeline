ğŸ“Œ ETL Automation Project â€” Medallion Architecture (Bronze â†’ Silver â†’ Gold)
ğŸ“– Project Overview

This project implements an end-to-end ETL data pipeline using Python + Pandas, designed using Medallion Architecture (Bronze, Silver, Gold layers).

The pipeline ingests raw industry-level dirty data, applies schema validation, data quality checks, transformations, and generates analytics-ready datasets.

This structure mirrors real-world data engineering pipelines used in modern data platforms.

ğŸ— Architecture â€” Medallion Flow
Raw CSV
   â†“
Bronze Layer â†’ Raw ingestion + schema validation
   â†“
Silver Layer â†’ Cleaning + standardization + business rules
   â†“
Gold Layer â†’ Aggregations + analytics-ready tables

Layer Responsibilities
Layer	Purpose
Bronze	Raw ingestion, schema enforcement, basic validation
Silver	Data cleaning, normalization, business rules
Gold	Aggregations, KPIs, analytics datasets
ğŸ“ Project Structure
etl_project/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ bronze.py
â”‚   â”‚
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â”œâ”€â”€ silver.py
â”‚   â”‚   â””â”€â”€ gold.py
â”‚   â”‚
â”‚   â”œâ”€â”€ validations/
â”‚   â”‚   â””â”€â”€ rules.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ config_loader.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â”‚
â”œâ”€â”€ logs/
â”œâ”€â”€ config.yaml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Tech Stack

Python 3.10+

Pandas

PyArrow (for parquet handling)

YAML

Logging module

ğŸ”„ Pipeline Execution Flow
Step 1 â€” Bronze Layer

Read raw CSV data

Apply schema validation

Store raw-clean parquet in bronze folder

Step 2 â€” Silver Layer

Handle nulls

Clean email & phone formats

Remove duplicates

Apply business validation rules

Save clean parquet dataset

Step 3 â€” Gold Layer

Perform aggregations

Create KPI metrics

Generate business-level analytical tables

â–¶ï¸ How To Run the Pipeline
1ï¸âƒ£ Create Virtual Environment (Recommended)
python -m venv venv
venv\Scripts\activate

2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run Pipeline
python src/main.py --input data/raw/industry_dirty_dataset.csv

ğŸ“Š Output Datasets
Layer	Path
Bronze	data/bronze/bronze_raw.parquet
Silver	data/silver/silver_clean.parquet
Gold	data/gold/gold_analytics.parquet

ğŸ§ª Validations Implemented

Schema validation

Null checks

Age boundary checks

Email format cleaning

Duplicate record handling

Business rule validation

ğŸªœ Future Enhancements

Delta Lake support

Spark-based distributed processing

Airflow orchestration

Data quality monitoring dashboards

CI/CD pipeline integration

ğŸ‘¨â€ğŸ’» Author

Swapnil

Data Engineering | ETL Automation | Analytics Pipelines
